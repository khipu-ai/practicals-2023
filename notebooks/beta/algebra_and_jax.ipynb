{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Algebra and JAX <a href=\"https://colab.research.google.com/github/khipu-ai/practicals-2023/blob/main/notebooks/algebra_and_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo.png\" width=\"25%\" />\n",
        "</div>\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this tutorial, we will introduce how to perform numeric computations, in particular matrix computations, which are the basis of most Machine Learning algorithms. We'll begin with the basics using the de facto standard NumPy, and then move on to **JAX**. \n",
        "\n",
        "JAX is a new library that allows for very efficient speedups in the aforementioned operations by transparently choosing the best available hardware (CPU, GPU, TPU, etc.) and provinding with a number of acceleration and automation mechanisms such as automatic differentiation (`grad`), parallelization (`pmap`), vectorization (`vmap`) and just-in-time compilation (`jit`).\n",
        "\n",
        "## Topics\n",
        "\n",
        "* Numeric computation and linear algebra with NumPy\n",
        "* Accelerated computation with JAX\n",
        "\n",
        "## Objectives\n",
        "\n",
        "* Learn to perform mathematical operations, in particular matrix operations in Python using  NumPy\n",
        "* Learn the basics of JAX and its similarities and differences with NumPy\n",
        "* Learn how to use JAX transforms - `jit`, `grad`, `vmap`, and `pmap`.\n",
        "\n",
        "## Outline\n",
        "\n",
        "1. [Introduction](#scrollTo=Enx0WUr8tIPf)\n",
        "1. [Similarities between JAX and NumPy](#scrollTo=CbOEYsWQ6tHv)\n",
        "1. [Differences between JAX and NumPy‚ùå](#scrollTo=lg4__l4A7yqc)\n",
        "1. [Acceleration in JAX üöÄ](#scrollTo=TSj972IWxTo2)\n",
        "1. [JAX is backend Agnostic](#scrollTo=_bQ9QqT-yKbs)\n",
        "1. [JAX Transformations](#scrollTo=JM_08mXEBRIK)\n",
        "1. [jit and grad](#scrollTo=cOGuGWtLmP7n)\n",
        "1. [Pure Functions](#scrollTo=fT56qxXzTVKZ)\n",
        "1. [vmap and pmap](#scrollTo=tvBzh8wiGuLf)\n",
        "\n",
        "## Licence\n",
        "\n",
        "* ¬© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "* ¬© Khipu 2023. Apache License 2.0.\n",
        "\n",
        "## Version history\n",
        "\n",
        "### 2022\n",
        "\n",
        "* **Authors:** \n",
        "  * Kale-ab Tessera\n",
        "  * Ignacio Ram√≠rez (nacho@fing.edu.uy)\n",
        "* **Reviewers:** \n",
        "  * Javier Antoran\n",
        "  * James Allingham\n",
        "  * Ruan van der Merwe\n",
        "  * Sebastian Bodenstein\n",
        "  * Laurence Midgley\n",
        "  *  Joao Guilherme\n",
        "  *  Elan van Biljon.   \n",
        "\n",
        "# Before you start\n",
        "\n",
        "For this practical, **you will need to use a GPU** to speed up training. \n",
        "To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4boGA9rYdt9l",
        "outputId": "4536697e-9991-4c0f-a59c-cd6c951d5356",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a GPU is connected.\n"
          ]
        }
      ],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import os\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "    # x8 cpu devices - number of (emulated) host devices\n",
        "    os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YQe1CfDyrkdL"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions. (Run Cell)\n",
        "import copy\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def plot_performance(data: Dict, title: str):\n",
        "    runs = list(data.keys())\n",
        "    time = list(data.values())\n",
        "\n",
        "    # creating the bar plot\n",
        "    plt.bar(runs, time, width=0.35)\n",
        "\n",
        "    plt.xlabel(\"Implementation\")\n",
        "    plt.ylabel(\"Average time taken (in s)\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    best_perf_key = min(data, key=data.get)\n",
        "    all_runs_key = copy.copy(runs)\n",
        "\n",
        "    # all_runs_key_except_best\n",
        "    all_runs_key.remove(best_perf_key)\n",
        "\n",
        "    for k in all_runs_key:\n",
        "        print(\n",
        "            f\"{best_perf_key} was {round((data[k]/data[best_perf_key]),2)} times faster than {k} !!!\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix computations with NumPy\n",
        "\n",
        "Below we introduce the basics of matrix and array manipulation and operation using the _de facto_ Python library for this task: NumPy. Feel free to skip if you are already familiar with all of this.\n",
        "\n",
        "In Python/Numpy, `array` objects generalize the concept of a (numeric) _matrix_ to any number of dimensions: vectors are _one-dimensional arrays_, in the sense that they are indexed by one single index. Matrices, in turn, are _two-dimensional arrays_, since we need two indexes to specify an element. In general, an N-dimensional array requires N indexes to specify any given value within it.\n",
        "\n",
        "Note that _dimension_ in this context is not the algebraic dimension of the object (in the case of a vector, its size), but the number of indexes that are needed to specifg an element. Let's see some examples:\n",
        "\n",
        "* A vector $x$ is specified as $x=(x_1,x_2,\\ldots,x_n)$. If $x_i \\in \\mathbb{R}$, we say that the vector $x \\in \\mathbb{R}^n$.\n",
        "* A $2{\\times}2$ matrix $A$ is given by\n",
        "$$\\left(\\begin{array}{cc}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{array}\\right).$$\n",
        "As an algebraic object, an $m{\\times}n$ matrix with real entries belongs to the space $\\mathbb{R}^{m{\\times}n}$.\n",
        "* The algebraic generalization of a matrix to objects with $N$ indexes is known as a _tensor_. For example, an $m{\\times}n{\\times}p$ tensor is given by $T=\\{t_{ijk}: 1 \\leq i \\leq m, 1 \\leq j \\leq n, 1 \\leq k \\leq k\\}$\n",
        "\n",
        "Although not part of the Python distribution, `NumPy` is the de-facto standard for matrix and tensor manipulation and operations, along with a large number of utilities including random number generation, basic statistics, linear transformations, and many more.\n",
        "\n",
        "Below we give a quick overview of the core NumPy interface, along with some basic Linear Algebra definitions.\n",
        "\n",
        "## Creation and basic access\n"
      ],
      "metadata": {
        "id": "ibp4dAvRk-Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print('CREATION\\n')\n",
        "#\n",
        "# 1. CREATION\n",
        "#\n",
        "x = np.array([1,2,3])\n",
        "print('x:',x)\n",
        "\n",
        "#\n",
        "# create a 2D array (a matrix):\n",
        "# the data is specified as a list of rows, all of the same size\n",
        "# each row is itself a list (or a tuple)\n",
        "#\n",
        "A = np.array( [ [3,4,5], [6,7,8], [9, 10, 11]] )\n",
        "print('A:\\n',A)\n",
        "\n",
        "#\n",
        "# we can also do this with a  tuple of tuples\n",
        "#\n",
        "B = np.array( ( (3,4,5), (6,7,8), (9,10,11) ) )\n",
        "print('\\nB:\\n',A)\n",
        "print()\n",
        "\n",
        "#\n",
        "# SINGLE ELEMENT ACCESS\n",
        "#\n",
        "print('\\n\\nREADING\\n')\n",
        "\n",
        "# in Python, array indexes begin at 0\n",
        "# so, the first element of a three element vector x is x[0], and the last is x[2]\n",
        "#\n",
        "print('first element of x:',x[0])\n",
        "print('last element of x:',x[2])\n",
        "#\n",
        "# Numpy follows the same indexing convention as Python, allowing us to acces elements\n",
        "# from end to start using negative indices. For example, the last element of x\n",
        "# can also be accessed as:\n",
        "#\n",
        "print('last element of x:',x[-1]) # notice it 'starts with' -1\n",
        "print('next-to-last element of x:',x[-2]) # notice it 'starts with' -1\n",
        "#\n",
        "# for matrices, we need to specify two indexes\n",
        "# the top-left element of a matrix has both indexes 0\n",
        "#\n",
        "print('top-left element of A:',A[0,0])\n",
        "print('bottom-right element of A:',A[-1,-1])\n",
        "#\n",
        "# WRITING A SINGLE VALUE\n",
        "#\n",
        "print('\\n\\nWRITING\\n')\n",
        "A = np.array( [ [3,4,5], [6,7,8], [9, 10, 11]] )\n",
        "print('A before:\\n',A)\n",
        "A[1,1] = 0\n",
        "print('\\nA after changing a single value:\\n',A)\n"
      ],
      "metadata": {
        "id": "_CdJt7EApRAR",
        "outputId": "80e8e77f-cee7-4c5a-e2d8-5ec1dc44c96f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATION\n",
            "\n",
            "x: [1 2 3]\n",
            "A:\n",
            " [[ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]]\n",
            "\n",
            "B:\n",
            " [[ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]]\n",
            "\n",
            "\n",
            "\n",
            "READING\n",
            "\n",
            "first element of x: 1\n",
            "last element of x: 3\n",
            "last element of x: 3\n",
            "next-to-last element of x: 2\n",
            "top-left element of A: 3\n",
            "bottom-right element of A: 11\n",
            "\n",
            "\n",
            "WRITING\n",
            "\n",
            "A before:\n",
            " [[ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]]\n",
            "\n",
            "A after changing a single value:\n",
            " [[ 3  4  5]\n",
            " [ 6  0  8]\n",
            " [ 9 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Element types\n",
        "\n",
        "NumPy can handle several types of elements. This is specified using the `dtype` optional argument at creation time. otherwise, the default `float64` type is used."
      ],
      "metadata": {
        "id": "dmh0AZ5XxT2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Afloat = np.ones((2,3))\n",
        "print('A matrix of real-valued ones (notice the decimal dot)\\n',Afloat)\n",
        "Aint = np.ones((2,3),dtype=int)\n",
        "print('A matrix of integer-valued ones\\n',Aint)\n",
        "Abool = np.ones((2,3),dtype=bool)\n",
        "print('A matrix of boolean-valued ones\\n',Abool)\n",
        "Abyte  = np.ones((2,3),dtype=np.uint8)\n",
        "print('A matrix of 8 bit unsigned integers, again all ones\\n',np.uint8) # this is a custom NumPy type, not built in in Python\n",
        "#\n",
        "# beware the semantics!!\n",
        "#\n",
        "print('\\nnegated boolean\\n',~Abool)\n",
        "print('\\nnegated bytes\\n',~Abyte)\n",
        "print('\\nsigned integer matrix - 10\\n',Aint-10) \n",
        "print('\\nunsigned 8 bit matrix - 10\\n',Abyte-10) # this is NOT -9! Why?\n"
      ],
      "metadata": {
        "id": "ifseQRkTxgFf",
        "outputId": "ef556afe-35dd-460f-c7df-41e78cf45eeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A matrix of real-valued ones (notice the decimal dot)\n",
            " [[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "A matrix of integer-valued ones\n",
            " [[1 1 1]\n",
            " [1 1 1]]\n",
            "A matrix of boolean-valued ones\n",
            " [[ True  True  True]\n",
            " [ True  True  True]]\n",
            "A matrix of 8 bit unsigned integers, again all ones\n",
            " <class 'numpy.uint8'>\n",
            "\n",
            "negated boolean\n",
            " [[False False False]\n",
            " [False False False]]\n",
            "\n",
            "negated bytes\n",
            " [[254 254 254]\n",
            " [254 254 254]]\n",
            "\n",
            "signed integer matrix - 10\n",
            " [[-9 -9 -9]\n",
            " [-9 -9 -9]]\n",
            "\n",
            "unsigned 8 bit matrix - 10\n",
            " [[247 247 247]\n",
            " [247 247 247]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating some useful matrices\n",
        "\n",
        "Numpy includes functions for creating some commonly occuring matrices.\n",
        "The semantics are some times confusing because the basic matrix creation functions expect a _single_ argument which is a tuple with the specified dimensions. However, there are some Numpy functions that take two arguments of the form `nrows,ncols`. We'll see about this later."
      ],
      "metadata": {
        "id": "EPBZvgsUbYph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z = np.zeros((10,4)) # the argument is a SINGLE TUPLE with the dimensions\n",
        "# Z = np.zeros(10,4) # THIS DOES NOT WORK! \n",
        "print('all zeroes\\n',Z)\n",
        "\n",
        "O = np.ones( (5,5))\n",
        "print('\\nall ones\\n',O)\n",
        "\n",
        "I = np.eye(6) # this is always square, so only one dimension is needed\n",
        "print('\\nidentity (eye)\\n',I)\n",
        "\n",
        "x = (1,2,3,4)\n",
        "D = np.diag(x) \n",
        "print('\\ndiagonal\\n',D)\n"
      ],
      "metadata": {
        "id": "G0NV0lO5blKU",
        "outputId": "3003f830-7ee1-4acc-dd36-f05a04a559ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all zeroes\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "all ones\n",
            " [[1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]]\n",
            "\n",
            "identity (eye)\n",
            " [[1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            "diagonal\n",
            " [[1 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 3 0]\n",
            " [0 0 0 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copying, shallow and deep\n",
        "\n",
        "One of the main culprits of obscure bugs for the inexperienced is when the semantics do not work as one would expect. \n",
        "\n",
        "Copying in Numpy, as in many libraries which handle possibly large objects, is by default a _shallow_ operation. This means that the statement `B=A` does not create a duplicate of `A`. Instead, `B` is just another handle/reference to the same data as `A`. A deep copy, where a new independent object is created, has to be done explicitly via functions.\n",
        "\n",
        "Still more trouble: there are _many_ other operations which do not create new data. As a general rule, everything that just reorders the matrix in a simple way (not an arbitrary permutation) will keep the same data. This includes _slicing_, _transposing_, _reshaping_ and _flattening_. We'll see about this later.\n"
      ],
      "metadata": {
        "id": "c6896XKCV8M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# shallow copy\n",
        "#\n",
        "A = np.array( [ [3,4,5], [6,7,8], [9, 10, 11]] )\n",
        "B = A\n",
        "A[1,1] = 33\n",
        "print('SHALLOW COPY\\n')\n",
        "print('A:\\n',A)\n",
        "print('B = A\\n',B)\n",
        "#\n",
        "# deep copy, new object\n",
        "#\n",
        "B = np.copy(A)\n",
        "A[1,1] = 1000\n",
        "print('\\n\\nDEEP COPY\\n')\n",
        "print('A:\\n',A)\n",
        "print('B (deep):\\n',B)\n",
        "#\n",
        "# deep copy into existing matrix (needs to be compatible!)\n",
        "#\n",
        "# this is very useful for saving memory and speeding up code!! \n",
        "#\n",
        "np.copyto(B,A) # order is: destination, source\n",
        "print('A:\\n',A)\n",
        "print('B after copyto:\\n',B)\n"
      ],
      "metadata": {
        "id": "n1ilZ3fqW75h",
        "outputId": "89e5a3c0-aa7c-4694-acb1-f7976a5e5ecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHALLOW COPY\n",
            "\n",
            "A:\n",
            " [[ 3  4  5]\n",
            " [ 6 33  8]\n",
            " [ 9 10 11]]\n",
            "B = A\n",
            " [[ 3  4  5]\n",
            " [ 6 33  8]\n",
            " [ 9 10 11]]\n",
            "\n",
            "\n",
            "DEEP COPY\n",
            "\n",
            "A:\n",
            " [[   3    4    5]\n",
            " [   6 1000    8]\n",
            " [   9   10   11]]\n",
            "B (deep):\n",
            " [[ 3  4  5]\n",
            " [ 6 33  8]\n",
            " [ 9 10 11]]\n",
            "A:\n",
            " [[   3    4    5]\n",
            " [   6 1000    8]\n",
            " [   9   10   11]]\n",
            "B after copyto:\n",
            " [[   3    4    5]\n",
            " [   6 1000    8]\n",
            " [   9   10   11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block access - Slicing\n",
        "\n",
        "Like the name implies, _slicing_ is the process whereby one takes a piece of a matrix and handles it separately. This of course is extremely useful. One can extract rows, columns or blocks using the semantics we show below.\n",
        "\n",
        "### Slicing and shallow/deep access\n",
        "\n",
        "A _very_ important aspect to remember: slicing **is a shallow operation**. You still need to make a deep copy if you don't want to change the original matrix!"
      ],
      "metadata": {
        "id": "95FLCbw0UvI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# we can cut chunks of vectors and matrices by specifying ranges\n",
        "#\n",
        "# \n",
        "print('\\n\\nSLICING\\n')\n",
        "\n",
        "x_sub = x[0:2] # first two elements of x\n",
        "x_sub = x[:2] # exactly equivalent; the 0 may be ommited\n",
        "x_sub = x[1:3] # last two elements of x \n",
        "x_sub = x[1:] # again, last index may be ommited\n",
        "x_sub = x[-2:] # last two elements, using negative indexes!\n",
        "print('subvector of x:',x_sub)\n",
        "x_fakesub = x[:] # this is 'all' the elements of x = x\n",
        "print('fakesub of x:',x_fakesub)\n",
        "\n",
        "#\n",
        "# matrix slicing\n",
        "#\n",
        "Asub = A[1:,1:] # lower-right 2x2 block of A\n",
        "print('Asub')\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# EXCERSISE: \n",
        "# extract the 2x2 submatrix of A using negative indexes!\n",
        "#  \n",
        "print('\\n\\nEXCERSISE 1')\n",
        "\n",
        "#\n",
        "#\n",
        "#-------------------------------------------------------------\n",
        "\n",
        "#\n",
        "# extracting rows and columns\n",
        "#\n",
        "# we saw that [:] means 'all the indexes'\n",
        "# then, if we specify an index for a row, and ':' for columns,\n",
        "# we get the entire row:\n",
        "Arow2 = A[1,:]\n",
        "# and if we want, for example, the first column:\n",
        "Acol1 = A[:,0]\n",
        "print('\\nsecond row of A:\\n',Arow2)\n",
        "print('\\nfirst column of A:\\n',Acol1)\n",
        "# -----------------------------------------------------------\n",
        "# EXCERSISE: \n",
        "# extract the last row of A using negative indexes\n",
        "#  \n",
        "print('\\n\\nEXCERSISE 2')\n",
        "\n",
        "#\n",
        "#\n",
        "#-------------------------------------------------------------\n",
        "#\n",
        "# WRITING MULTIPLE ELEMENTS\n",
        "#\n",
        "print('\\n\\nWRITING TO SLICES\\n')\n",
        "A[:,1] = 33\n",
        "print('A after setting many values:\\n',A)\n",
        "#\n",
        "# pasting other data\n",
        "#\n",
        "a = np.array( (44,44) )\n",
        "A[0,1:3] = a\n",
        "print('A after pasting a vector:\\n',A)\n",
        "#\n",
        "# slicing does NOT create new data!!\n",
        "#\n",
        "Asub = A[:2,:2]\n",
        "print('Taking a slice of A:\\n',Asub)\n",
        "Asub[:,:] = 0 # set all elements to 0\n",
        "print('Slice, after zeroing:\\n',Asub)\n",
        "print('A after zeroing the slice:\\n',A)\n"
      ],
      "metadata": {
        "id": "qZ2WS3CvU0OB",
        "outputId": "3cf923cd-435d-4416-f282-394af3f5ce6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "SLICING\n",
            "\n",
            "subvector of x: (3, 4)\n",
            "fakesub of x: (1, 2, 3, 4)\n",
            "Asub\n",
            "\n",
            "\n",
            "EXCERSISE 1\n",
            "\n",
            "second row of A:\n",
            " [   6 1000    8]\n",
            "\n",
            "first column of A:\n",
            " [3 6 9]\n",
            "\n",
            "\n",
            "EXCERSISE 2\n",
            "\n",
            "\n",
            "WRITING TO SLICES\n",
            "\n",
            "A after setting many values:\n",
            " [[ 3 33  5]\n",
            " [ 6 33  8]\n",
            " [ 9 33 11]]\n",
            "A after pasting a vector:\n",
            " [[ 3 44 44]\n",
            " [ 6 33  8]\n",
            " [ 9 33 11]]\n",
            "Taking a slice of A:\n",
            " [[ 3 44]\n",
            " [ 6 33]]\n",
            "Slice, after zeroing:\n",
            " [[0 0]\n",
            " [0 0]]\n",
            "A after zeroing the slice:\n",
            " [[ 0  0 44]\n",
            " [ 0  0  8]\n",
            " [ 9 33 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding and removing rows/columns\n",
        "\n",
        "There are two ways in which you can change a matrix: either you add/remove columns/rows, or you reshape it.\n",
        "\n",
        "There is a large difference between these two groups of operations. NumPy does not add or delete stuff from a matrix _in place_. When you do so, a new array is returned _always_.\n",
        "\n",
        "On the other hand, changing the shape without changing the total number of elements does not create new data, but just a new _view_ to that data.\n"
      ],
      "metadata": {
        "id": "9xRST2x6VhrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# ADDING AND REMOVING ROWS/COLUMNS\n",
        "#\n",
        "#\n",
        "# deleting is _not_ a common operation\n",
        "# in matrix manipulation. \n",
        "#\n",
        "# 'delete' a row: A is unchanged!\n",
        "#\n",
        "A = np.eye(4)\n",
        "print('A before deleting the third row\\n',A)\n",
        "B = np.delete(A,2,axis=0) # axis: 0 means 'a row', 1 means 'a column'\n",
        "print('A after deleting the third row\\n',A)\n",
        "print('B after deleting the third row\\n',B)\n",
        "#\n",
        "# insertion: same thing: A is unchanged\n",
        "#\n",
        "# arguments: source matrix, where, what and which axis\n",
        "#\n",
        "B = np.insert(A,0, np.zeros((4)),axis=1) \n",
        "print('A after inserting a column of zeroes to the left\\n',A)\n",
        "print('B after inserting a column of zeroes to the left\\n',B)\n",
        "#\n",
        "# append: insert at the end \n",
        "# this is actually tricky: the number of dimensions of the object appended\n",
        "# must match those of the source matrix, and the shapes along those dimensions\n",
        "# (with the except of the specified axis) must be the same too\n",
        "#\n",
        "print(np.ones(4)) # a flat vector (one dimension)\n",
        "# B = np.append(A,np.ones((4)),axis=0) # DOES NOT WORK: source has two dimensions (matrix)\n",
        "# B = np.append(A,np.ones((4,1)),axis=1) # DOES NOT WORK: shapes not compatible\n",
        "B = np.append(A,np.ones((4,1)),axis=1) # WORKS: source has two dimensions (matrix)\n",
        "\n",
        "print('A after appending a column of ones\\n',A) # as always, A does not change\n",
        "print('B after appending a column of ones\\n',B)\n",
        "#\n",
        "# you can remove/insert/append  slices too\n",
        "#\n",
        "B = np.append(A,np.ones((4,3)),axis=1) # add three columns\n",
        "print('B after appending three columns of ones\\n',B)\n",
        "#\n",
        "# CONCATENATION\n",
        "# \n",
        "# you can concatenate any number of matrices along a given axis\n",
        "# remember: axis=0 means 'rows' and axis=1 means 'columns'\n",
        "# so if you want to concatenate side by side, it is axis=1\n",
        "# if you want to stack one above the other, it is axis=0\n",
        "A = np.zeros((5,3)) # same number of columns\n",
        "B = np.ones((4,3))\n",
        "print('\\nCONCATENATION\\n')\n",
        "print('A\\n',A,'B\\n',B)\n",
        "source_matrices_tuple = (A,B) # needs to be a tuple\n",
        "# AB = np.concatenate(source_matrices_tuple, axis=1) # DOES NOT WORK: not the same number of rows!\n",
        "AB = np.concatenate(source_matrices_tuple, axis=0) # WORKS: same number of columns\n",
        "print('A over B\\n',AB)\n"
      ],
      "metadata": {
        "id": "QkeQwPhOttMX",
        "outputId": "794255a3-04cb-4a15-9df8-d035e3352cbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A before deleting the third row\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "A after deleting the third row\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "B after deleting the third row\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "A after inserting a column of zeroes to the left\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "B after inserting a column of zeroes to the left\n",
            " [[0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n",
            "[1. 1. 1. 1.]\n",
            "A after appending a column of ones\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "B after appending a column of ones\n",
            " [[1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 1. 1.]]\n",
            "B after appending three columns of ones\n",
            " [[1. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 1. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 1. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1. 1. 1.]]\n",
            "\n",
            "CONCATENATION\n",
            "\n",
            "A\n",
            " [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]] B\n",
            " [[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "A over B\n",
            " [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Raveling, reshaping, transposing and flipping\n",
        "\n",
        "You can change the way elements are shown without changing the data itself. In other words, these are **shallow** operations which create _views_ to the original data.\n",
        "\n",
        "When reshaping, one must understand how the data is stored in memory. NumPy follows the so called 'C' ordering of matrices, also known as 'row major'. This means that the main (major) dimension is the row, and the secondary is the column. \n",
        "\n",
        "A good way to correctly predict the elements in the reshaped data is to first _flatten_ the data. This is actually one of the basic views, which can be obtained using the `ravel` operation.\n",
        "\n",
        "Then, for any given shape that we may cast the original matrix to, the data will flow along the last dimension first, then the second to last, etc. This is much easier to see with examples.\n"
      ],
      "metadata": {
        "id": "jig7A9pmmffc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# raveling\n",
        "#\n",
        "A = np.array([[1,2,3],[4,5,6]]) # 6 elements\n",
        "print('A\\n',A,'\\nflattened A\\n',A.ravel())\n",
        "#\n",
        "# reshaping\n",
        "#\n",
        "\n",
        "B = np.reshape(A,(3,2)) # reshaped must have the same number of elements!\n",
        "print('\\nreshaped to 3x2\\n',B)\n",
        "#\n",
        "# transposing\n",
        "#\n",
        "B = np.transpose(A) # reshaped must have the same number of elements!\n",
        "B = A.T # shortcut\n",
        "print('\\ntransposed \\n',B)\n",
        "B[2,1] = 0\n",
        "print('\\nzeroed an element in transposed \\n',B)\n",
        "print('\\nshallow: A is also changed \\n',A)\n",
        "#\n",
        "# flipping\n",
        "#\n",
        "B = np.fliplr(A) # reverse ordering of columns\n",
        "print('\\nA, columns reversed\\n',B)\n",
        "B[1,0] = 33\n",
        "print('\\nchanged B\\n',B)\n",
        "print('\\nalso shallow: A is also changed \\n',A)\n"
      ],
      "metadata": {
        "id": "_E_PLC90t38Z",
        "outputId": "2c1c0223-4cac-4b24-bc88-993cc24edc76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            " [[1 2 3]\n",
            " [4 5 6]] \n",
            "flattened A\n",
            " [1 2 3 4 5 6]\n",
            "\n",
            "reshaped to 3x2\n",
            " [[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "\n",
            "transposed \n",
            " [[1 4]\n",
            " [2 5]\n",
            " [3 6]]\n",
            "\n",
            "zeroed an element in transposed \n",
            " [[1 4]\n",
            " [2 5]\n",
            " [3 0]]\n",
            "\n",
            "shallow: A is also changed \n",
            " [[1 2 3]\n",
            " [4 5 0]]\n",
            "\n",
            "A, columns reversed\n",
            " [[3 2 1]\n",
            " [0 5 4]]\n",
            "\n",
            "changed B\n",
            " [[ 3  2  1]\n",
            " [33  5  4]]\n",
            "\n",
            "also shallow: A is also changed \n",
            " [[ 1  2  3]\n",
            " [ 4  5 33]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked access\n",
        "\n",
        "Instead of specifying indexes, one may supply a binary mask of `0`s and `1`s and extract those elements where the mask s `1`. \n",
        "The values returned have two important properties: first, they are raveled (it is the only way to represent arbitrarily chosen elements from a matrix) and, second, they are still a **shallow** view of the original matrix!\n"
      ],
      "metadata": {
        "id": "nX379yyav90_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# masking\n",
        "#\n",
        "A = np.array([[1,2,3],[4,5,6]]) # 6 elements\n",
        "M = np.array([[1,0,0],[0,1,0]],dtype=bool) # binary mask\n",
        "print('A\\n',A,'\\nmask\\n',M)\n",
        "print('elements of A extracted by the mask (raveled!)\\n',A[M])\n",
        "A[M]=1000\n",
        "print('change masked elements: this is also a shallow operation!\\n',A)\n"
      ],
      "metadata": {
        "id": "pygeJQQhwOKa",
        "outputId": "207b332b-1e89-464e-e514-71a51ee79a50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            " [[1 2 3]\n",
            " [4 5 6]] \n",
            "mask\n",
            " [[ True False False]\n",
            " [False  True False]]\n",
            "elements of A extracted by the mask (raveled!)\n",
            " [1 5]\n",
            "change masked elements: this is also a shallow operation!\n",
            " [[1000    2    3]\n",
            " [   4 1000    6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix-Vector Algebra\n",
        "\n",
        "Here we will see how to perform matrix-matrix and matrix-vector products.\n",
        "Care must also be taken: the operator `*` is _not_ for these tasks! There are special functions and operations for performing algebraic operations. We'll see this below.\n"
      ],
      "metadata": {
        "id": "pBYKFqnYpoN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#\n",
        "#\n",
        "A = np.array([[1,-1,0],[-1,1,0],[0,1,-1],[0,-1,1]])\n",
        "x = np.array([1,4,9])\n",
        "print('A\\n',A,'\\nx\\n',x)\n",
        "#\n",
        "# this is _not_ a dot product. What does it do?\n",
        "#\n",
        "print('\\nx * x\\n', x * x)\n",
        "print('\\nA * x\\n', A * x)\n",
        "#\n",
        "# this is a dot product\n",
        "#\n",
        "print('\\nAx\\n',A @ x)\n",
        "print('\\n<x,x>\\n', x @ x)\n",
        "print('\\n<x,x>\\n', x @ x.T) # for vectors, its always the _inner_ product\n",
        "print('\\n<x,x>\\n', x.T @ x) # transposition does not count\n",
        "#\n",
        "# old (but safest), using functions\n",
        "#\n",
        "print('\\nAx\\n',np.dot(A,x))\n",
        "print('\\nxtx\\n',np.dot(x,x))\n",
        "# print('\\nAx\\n',np.dot(A,A)) DOES NOT WORK!  Why?\n",
        "print('\\nAAt\\n',np.dot(A,A.T)) # for matrices, transposition _counts_\n"
      ],
      "metadata": {
        "id": "OJ4Mwzqsp7sm",
        "outputId": "555dfa26-688b-4846-d779-a6f477171bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            " [[ 1 -1  0]\n",
            " [-1  1  0]\n",
            " [ 0  1 -1]\n",
            " [ 0 -1  1]] \n",
            "x\n",
            " [1 4 9]\n",
            "\n",
            "x * x\n",
            " [ 1 16 81]\n",
            "\n",
            "A * x\n",
            " [[ 1 -4  0]\n",
            " [-1  4  0]\n",
            " [ 0  4 -9]\n",
            " [ 0 -4  9]]\n",
            "\n",
            "Ax\n",
            " [-3  3 -5  5]\n",
            "\n",
            "<x,x>\n",
            " 98\n",
            "\n",
            "<x,x>\n",
            " 98\n",
            "\n",
            "<x,x>\n",
            " 98\n",
            "\n",
            "Ax\n",
            " [-3  3 -5  5]\n",
            "\n",
            "xtx\n",
            " 98\n",
            "\n",
            "AAt\n",
            " [[ 2 -2 -1  1]\n",
            " [-2  2  1 -1]\n",
            " [-1  1  2 -2]\n",
            " [ 1 -1 -2  2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix and vector norms\n",
        "\n",
        "There is a sub-package of NumPy for this called `linalg`. There you'll find many advanced algebraic operations including norms, condition numbers, inverses, solving linear systems, QR, SVD, Cholesky, etc.\n"
      ],
      "metadata": {
        "id": "WxOVeSMYsBoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy.linalg as la \n",
        "\n",
        "A = np.array([[1,-1,0],[-1,1,0],[0,1,-1],[0,-1,1]])\n",
        "x = np.array([1,4,9])\n",
        "print('||x||_2:',la.norm(x)) # L2 norm\n",
        "print('||A||_F:',la.norm(A)) # default: Frobenius norm: norm of the 'unrolled' matrix as a vector\n",
        "print('||A||_F (again):',la.norm(A,'fro')) # explicitly say we want the frobenius (always good idea)\n",
        "print('||A||_2 (nuclear):',la.norm(A,'nuc')) # nuclear or spectral norm\n",
        "A = 4*np.eye(4)\n",
        "print('A\\n',A,'\\ninv(A)\\n',la.inv(A)) # must be square, of course"
      ],
      "metadata": {
        "id": "cW4BlvqxshCp",
        "outputId": "1730551f-5722-4f86-c79c-11c5bf60f451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||x||_2: 9.899494936611665\n",
            "||A||_F: 2.8284271247461903\n",
            "||A||_F (again): 2.8284271247461903\n",
            "||A||_2 (nuclear): 3.8637033051562732\n",
            "A\n",
            " [[4. 0. 0. 0.]\n",
            " [0. 4. 0. 0.]\n",
            " [0. 0. 4. 0.]\n",
            " [0. 0. 0. 4.]] \n",
            "inv(A)\n",
            " [[0.25 0.   0.   0.  ]\n",
            " [0.   0.25 0.   0.  ]\n",
            " [0.   0.   0.25 0.  ]\n",
            " [0.   0.   0.   0.25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other NumPy functions\n",
        "\n",
        "Besides matrices and algebra, NumPy brings many useful utilities. A particularly useful one is the package `numpy.random` which implements high quality pseudo-random number generators, and many functions, for example:\n",
        "\n",
        "* drawing random samples from various distributions\n",
        "* random subsamples of sets\n",
        "* random permutations\n"
      ],
      "metadata": {
        "id": "HPOh7VgbuBMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy.random as rand\n",
        "\n",
        "#\n",
        "# extremely important for reproducible research: set the seed!\n",
        "#\n",
        "rng = rand.default_rng(12345) \n",
        "#\n",
        "# draw some Normal samples\n",
        "#\n",
        "print('A 3x2 random with normal(0,1) samples\\n',rng.normal(size=(3,2)))\n",
        "#\n",
        "# very useful function for creating ranges, linear and logarithmic\n",
        "#\n",
        "x = np.arange(10)\n",
        "print('\\nx',x)\n",
        "print('random permutation of x',rng.permutation(x))\n",
        "print('random subsample of x',rng.choice(x,4))"
      ],
      "metadata": {
        "id": "sMlnbmDRuf5i",
        "outputId": "c726db8f-0ff1-49fa-8420-87b7afe7d81e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A 3x2 random with normal(0,1) samples\n",
            " [[-1.42382504  1.26372846]\n",
            " [-0.87066174 -0.25917323]\n",
            " [-0.07534331 -0.74088465]]\n",
            "\n",
            "x [0 1 2 3 4 5 6 7 8 9]\n",
            "random permutation of x [0 8 3 7 2 6 9 5 4 1]\n",
            "random subsample of x [4 0 8 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are done with the basics of NumPy. Of course there is a lot more. Check out the official [Numpy API](https://numpy.org/doc/stable/reference/index.html) to see what's there.\n",
        "\n",
        "We will now move onto JAX. JAX has two purposes: on one side, it provides a backend-transparent accelerated linear algebra interface which can run on specialized hardware such as GPUs and TPUs.\n",
        "\n",
        "It also provides _transformations_ for, example, parallel execution, or just-in-time compilation for maximum speed.\n"
      ],
      "metadata": {
        "id": "fpDkwTa0tvrj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enx0WUr8tIPf"
      },
      "source": [
        "\n",
        "# JAX\n",
        "\n",
        "[JAX](https://jax.readthedocs.io/en/latest/index.html) is a python package for writing composable numerical transformations. It leverages [Autograd](https://github.com/hips/autograd) and [XLA](https://www.tensorflow.org/xla) (Accelerated Linear Algebra), to achieve high-performance numerical computing, which is particularly relevant in machine learning.\n",
        "\n",
        "It provides functionality such as automatic differentiation (`grad`), parallelization (`pmap`), vectorization (`vmap`), just-in-time compilation (`jit`), and more. These transforms operate on [pure functions](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions), so JAX encourages a **functional programming** paradigm. Furthermore, the use of XLA  allows one to target different kinds of accelerators (CPU, GPU and TPU), without code changes.  \n",
        "\n",
        "JAX is different from frameworks such as PyTorch or Tensorflow (TF). It is more low-level and minimalistic. JAX simply offers a set of primitives (simple operations) like `jit` and `vmap`, and relies on other libraries for other things e.g. using the data loader from PyTorch or TF. Due to JAX's simplicity, it is commonly used with higher-level neural network libraries such as [Haiku](https://github.com/deepmind/dm-haiku) or [Flax](https://github.com/google/flax). (Imagine writing complicated architectures using a NumPy-like interface alone! üòÆ )   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yFzjRHUsUQqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6eb9a2c-e458-45ad-862a-12ce81c464d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num devices: 1\n",
            " Devices: [GpuDevice(id=0, process_index=0)]\n"
          ]
        }
      ],
      "source": [
        "## @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbOEYsWQ6tHv"
      },
      "source": [
        "# JAX vs NumPy \n",
        "\n",
        "## Similarities  ü§ù\n",
        "\n",
        "The main similarity between JAX and NumPy is that they share a similar interface and often, JAX and NumPy arrays can be used interchangeably. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McStJC-l3qsG"
      },
      "source": [
        "## Similiar Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbYfoaujT2F7"
      },
      "source": [
        "Let's plot the sine functions using NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sgRLq58OTz1t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "05d2254c-1078-4efd-d527-7f123b4ba6e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 100 linearly spaced numbers from -np.pi to np.pi\n",
        "x = np.linspace(-np.pi, np.pi, 100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = np.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=sin(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCEnlC-PU3ps"
      },
      "source": [
        "Now using jax. We already imported `jax.numpy` as `jnp` in the first cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kRQf2mNRTlt3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "ac45f481-ceb2-4cd3-fd7f-6fe2604c43ce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 100 linearly spaced numbers from -jnp.pi to jnp.pi\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = jnp.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=sin(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNscwHeV_dn"
      },
      "source": [
        "**Exercise 1.1 - Code Task:** Can you plot the cosine function using `jnp`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5svZFPUCQNsG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "6053fefe-cd10-4be5-ebdb-74ce88e0284c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f6c2fab0d581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# plot the functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y=cos(x)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1650\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_line%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1871\u001b[0m         \"\"\"\n\u001b[0;32m-> 1872\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1873\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mxconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xorig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'ellipsis'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot Cosine using jnp. (UPDATE ME)\n",
        "\n",
        "# 100 linearly spaced numbers\n",
        "# UPDATE ME\n",
        "x = ...\n",
        "\n",
        "# UPDATE ME\n",
        "y = ...\n",
        "\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=cos(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4AVrGzy6JWR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "# 100 linearly spaced numbers\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "y = jnp.cos(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=cos(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg4__l4A7yqc"
      },
      "source": [
        "## Differences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPbOnhE4ZSTi"
      },
      "source": [
        "## JAX arrays are immutable, NumPy arrays are not\n",
        "\n",
        "JAX and NumPy arrays are often interchangeable, **but** Jax arrays are **immutable** (they can't be modified after they are created). Allowing mutations makes transforms difficult and violates conditions for [pure functions](https://en.wikipedia.org/wiki/Pure_function).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumPy\n",
        "\n",
        "Let's see this in practice by changing the number at the beginning of an array. "
      ],
      "metadata": {
        "id": "Vdfb1wtd-GkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r-Los6YZR-f"
      },
      "outputs": [],
      "source": [
        "# NumPy: mutable arrays\n",
        "x = np.arange(10)\n",
        "x[0] = 10\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX"
      ],
      "metadata": {
        "id": "8Y23OWjE_BDA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxjkKpqAZxWo"
      },
      "outputs": [],
      "source": [
        "# JAX: immutable arrays\n",
        "# Should raise an error.\n",
        "try:\n",
        "    x = jnp.arange(10)\n",
        "    x[0] = 10\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoWT5RBUagW8"
      },
      "source": [
        "So it fails! We can't mutate a JAX array once it has been created. To update JAX arrays, we need to use [helper functions](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html) that return an updated copy of the JAX array. \n",
        "\n",
        "Instead of doing this `x[idx] = y`, we need to do this `x = x.at[idx].set(y)`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJYxkh4qagwO"
      },
      "outputs": [],
      "source": [
        "x = jnp.arange(10)\n",
        "new_x = x.at[0].set(10)\n",
        "print(f\" new_x: {new_x} original x: {x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut0meCGB5qD0"
      },
      "source": [
        "Note here that `new_x` is a copy and that the original `x` is unchanged. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAH4c_smdGQU"
      },
      "source": [
        "## Random number generation in NumPy and JAX \n",
        "\n",
        "JAX is more explicit in Pseudo Random Number Generation (PRNG) than `NumPy` and other libraries (such as `TensorFlow` or `PyTorch`). [PRNG](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) is the process of algorithmically generating a sequence of numbers, which *approximate* the properties of a sequence of random numbers.  \n",
        "\n",
        "Let's see the differences in how JAX and NumPy generate random numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2m376Ethf8m"
      },
      "source": [
        "##### In Numpy, PRNG is based on a global `state`.\n",
        "\n",
        "Let's set the initial seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0t3sjxzdgmP"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "prng_state = np.random.get_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QKVz5atZMMOV"
      },
      "outputs": [],
      "source": [
        "# @title Helper function to compare prng keys (Run Cell)\n",
        "def is_prng_state_the_same(prng_1, prng_2):\n",
        "    \"\"\"Helper function to compare two prng keys.\"\"\"\n",
        "    # concat all elements in prng tuple\n",
        "    list_prng_data_equal = [(a == b) for a, b in zip(prng_1, prng_2)]\n",
        "    # stack all elements together\n",
        "    list_prng_data_equal = np.hstack(list_prng_data_equal)\n",
        "    # check if all elements are the same\n",
        "    is_prng_equal = all(list_prng_data_equal)\n",
        "    return is_prng_equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nloZ9abah3J3"
      },
      "source": [
        "Let's take a few samples from a Gaussian (normal) Distribution and check if PRNG keys/global state change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiUcfX7iSenY"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"sample 1 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 2 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 3 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuHkW6V4iLa9"
      },
      "source": [
        "Numpy's global random state is updated every time a random number is generated, so *sample 1 != sample 2 != sample 3*. \n",
        "\n",
        "Having the state automatically updated, makes it difficult to handle randomness in a **reproducible** way across different threads, processes and devices. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGDU6ckKkzqL"
      },
      "source": [
        "##### In JAX, PRNG is explicit.\n",
        "\n",
        "In JAX, for each random number generation, you need to explicitly pass in a random key/state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oKdk5CSmD-f"
      },
      "source": [
        "Passing the same state/key results in the same number being generated. This is generally undesirable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-6B0hjtlTmd"
      },
      "outputs": [],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "print(f\"sample 2 = {random.normal(key)}\")\n",
        "print(f\"sample 3 = {random.normal(key)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0KcwEbZqIaQ"
      },
      "source": [
        "To generate different and independent samples, you need to manually **split** the keys. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-7BhY0MmEhI"
      },
      "outputs": [],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "\n",
        "# We split the key -> new key and subkey\n",
        "new_key, subkey = random.split(key)\n",
        "\n",
        "# We use the subkey immediately and keep the new key for future splits.\n",
        "# It doesn't really matter which key we keep and which one we use immediately.\n",
        "print(f\"sample 2 = {random.normal(subkey)}\")\n",
        "\n",
        "# We split the new key -> new key2 and subkey\n",
        "new_key2, subkey = random.split(new_key)\n",
        "print(f\"sample 3 = {random.normal(subkey)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VnTDptmuk-i"
      },
      "source": [
        "By using JAX, we can more easily reproduce random number generation in parallel across threads, processes, or even devices by explicitly passing and keeping track of the prng key (without relying on a global state that automatically gets updated). For more details on PRNG in JAX, you can read more [here](https://jax.readthedocs.io/en/latest/jep/263-prng.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSj972IWxTo2"
      },
      "source": [
        "# Acceleration in JAX üöÄ\n",
        "\n",
        "JAX leverages Autograd and XLA for accelerating numerical computation. The use of Autograd allows for automatic differentiation (`grad`), while XLA allows JAX to run on multiple accelerators/backends and run transforms like `jit` and `pmap`. JAX also allows you to use `vmap` for automatic vectorization.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bQ9QqT-yKbs"
      },
      "source": [
        "## JAX is backend Agnostic\n",
        "\n",
        "Using JAX, you can run the same code on different backends/AI accelerators (e.g. CPU/GPU/TPU), **with no changes in code** (no more `.to(device)` - from frameworks like PyTorch). This means we can easily run linear algebra operations directly on GPU/TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbcFsfAibBu"
      },
      "source": [
        "**Multiplying Matrices**\n",
        "\n",
        "Dot products are a common operation in numerical computing and a central part of modern deep learning. They are defined over [vectors](https://en.wikipedia.org/wiki/Coordinate_vector), which can loosely be thought of as a list of multiple scalers (single values). \n",
        "\n",
        "Formally, given two vectors $\\boldsymbol{x}$,$\\boldsymbol{y}$ $\\in R^n$, their dot product is defined as:\n",
        "\n",
        "<center>$\\boldsymbol{x}^{\\top} \\boldsymbol{y}=\\sum_{i=1}^{n} x_{i} y_{i}$</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY1RsVkXaokP"
      },
      "source": [
        "Dot Product in NumPy (will run on cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj59KkD_HDOs"
      },
      "outputs": [],
      "source": [
        "size = 1000\n",
        "x = np.random.normal(size=(size, size))\n",
        "y = np.random.normal(size=(size, size))\n",
        "numpy_time = %timeit -o -n 10 a_np = np.dot(y,x.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c_kl-u0KPVY"
      },
      "source": [
        "Dot Product using JAX (will run on current runtime - e.g. GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHRcHK86KO3w"
      },
      "outputs": [],
      "source": [
        "size = 1000\n",
        "key1, key2 = jax.random.split(jax.random.PRNGKey(42), num=2)\n",
        "x = jax.random.normal(key1, shape=(size, size))\n",
        "y = jax.random.normal(key2, shape=(size, size))\n",
        "jax_time = %timeit -o -n 10 jnp.dot(y, x.T).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMTSpEG3TNah"
      },
      "source": [
        "\n",
        "> When timing JAX functions, we use `.block_until_ready()` because JAX uses [asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html#async-dispatch). This means JAX doesn't wait for the operation to complete before returning control to your code. To fairly compute the time taken for JAX operations, we therefore block until the operation is done.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3vwh6Q724gn"
      },
      "source": [
        "How much faster was the dot product in JAX (Using GPU)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkASX9p34A1D"
      },
      "outputs": [],
      "source": [
        "np_average_time = np.mean(numpy_time.all_runs)\n",
        "jax_average_time = np.mean(jax_time.all_runs)\n",
        "data = {\"numpy\": np_average_time, \"jax\": jax_average_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken per framework to run dot product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX not running much faster? -> Re-run the JAX cell.                       \n",
        "> \"Keep in mind that the first time you run JAX code, it will be slower because it is being compiled. T*his is true even if you don‚Äôt use jit in your own code, because JAX‚Äôs builtin functions are also jit compiled*.\" - [JAX Docs](https://jax.readthedocs.io/en/latest/faq.html#benchmarking-jax-code).\n",
        "\n",
        "If you are running on an accelerator, you should see a considerable performance benefit of using JAX, without making any changes to your code! \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X6Rv_OQgBOqr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_08mXEBRIK"
      },
      "source": [
        "# JAX Transformations\n",
        "\n",
        "JAX transforms (`jit`, `grad`, `vmap`, `pmap`) first convert python functions into an intermediate language called *jaxpr*. Transforms are then applied to this jaxpr representation.\n",
        "\n",
        "JAX generates jaxpr, in a process known as **tracing**. During tracing, function inputs are wrapped by a tracer object and then JAX records all operations (including regular python code) that occur during the function call. These recorded operations are used to reconstruct the function. \n",
        "\n",
        "Any python side-effects are not recorded during tracing. JAX transforms and compilations are designed to work only with **pure functions**. For more on tracing and jaxpr, you can read [here](https://jax.readthedocs.io/en/latest/jaxpr.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsJE_U-ZzVol"
      },
      "source": [
        "## JIT (Just in Time) compilation\n",
        "\n",
        "Jax dispatches operations to accelerators one at a time. If we have repeated operations, we can use `jit` to compile the function the first time it is called, then subsequent calls will be [cached](https://en.wikipedia.org/wiki/Cache_(computing) (save the compiled version so that it doesn't need to be recompiled everytime we call it). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIYsqIp_-Dly"
      },
      "source": [
        "Let's compile [ReLU (Rectified Linear Unit)](https://arxiv.org/abs/1803.08375), a popular activation function in deep learning. \n",
        "\n",
        "ReLU is defined as follows:\n",
        "<center>$f(x)=max(0,x)$</center>\n",
        "\n",
        "It can be visualized as follows:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"35%\" />\n",
        "</center>,\n",
        "\n",
        "where $x$ is the input to the function and $y$ is output of ReLU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm-bN9sQETLV"
      },
      "source": [
        "$$f(x)=\\max (0, x)=\\left\\{\\begin{array}{l}x_{i} \\text { if } x_{i}>0 \\\\ 0 \\text { if } x_{i}<=0\\end{array}\\right.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFiuu3BFAKdY"
      },
      "source": [
        "**Exercise 1.2 - Code Task:** Complete the ReLU implementation below using standard python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_qMJJbs-Cbe"
      },
      "outputs": [],
      "source": [
        "# Implement ReLU.\n",
        "def relu(x):\n",
        "    if x > 0:\n",
        "        return\n",
        "        # TODO Implement me!\n",
        "    else:\n",
        "        return\n",
        "        # TODO Implement me!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zCobLakM1esy"
      },
      "outputs": [],
      "source": [
        "# @title Run to test your function.\n",
        "\n",
        "\n",
        "def plot_relu(relu_function):\n",
        "    max_int = 5\n",
        "    # Generete 100 evenly spaced points from -max_int to max_int\n",
        "    x = np.linspace(-max_int, max_int, 1000)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    plt.plot(x, y, label=\"ReLU\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xticks(np.arange(min(x), max(x) + 1, 1))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def check_relu_function(relu_function):\n",
        "    # Generete 100 evenly spaced points from -100 to -1\n",
        "    x = np.linspace(-100, -1, 100)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    assert (y == 0).all()\n",
        "\n",
        "    # Check if x == 0\n",
        "    x = 0\n",
        "    y = relu_function(x)\n",
        "    assert y == 0\n",
        "\n",
        "    # Generete 100 evenly spaced points from 0 to 100\n",
        "    x = np.linspace(0, 100, 100)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    assert np.allclose(x, y)\n",
        "\n",
        "    print(\"Your ReLU function is correct!\")\n",
        "\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kken6_XvDdOK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def relu(x):\n",
        "    if x > 0:\n",
        "        return x\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mgIAyE2Fx3O"
      },
      "source": [
        "Let's try to `jit` this function to speed up compilation and then try to call it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YDkiNlRF6jn"
      },
      "outputs": [],
      "source": [
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "# Gen 1000000 random numbers and pass them to relu\n",
        "num_random_numbers = 1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "# Should raise an error.\n",
        "try:\n",
        "    relu_jit(x)\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7q33C4pHOQW"
      },
      "source": [
        "**Why does this fail?**\n",
        "\n",
        "\n",
        "> As mentioned above, JAX transforms first converts python functions into an intermediate language called *jaxpr*. Jaxpr only captures what is executed on the parameters given to it during tracing, so this means during conditional calls, jaxpr only considers the branch taken.\n",
        "> \n",
        "> When jit-compiling a function, we want to compile and cache a version of the function that can handle multiple different argument types (so we don't have to recompile for each function evaluation). For example, when we compile a function on an array `jnp.array([1., 2., 3.], jnp.float32)`, we would likely also want to use the compiled function for `jnp.array([4., 5., 6.], jnp.float32)`. \n",
        "> \n",
        "> To achieve this, JAX traces your code based on abstract values. The default abstraction level is a ShapedArray - array that has a fixed size and dtype, for example, if we trace a function using `ShapedArray((3,), jnp.float32)`,  it can be reused for any concrete array of size 3, and float32 dtype. \n",
        "> \n",
        "> This does come with some challenges. Tracing that relies on concrete values becomes tricky and sometimes results in `ConcretizationTypeError` as in the ReLU function above. Furthermore, when tracing a function with conditional statements (\"if ...\"), JAX doesn't know which branch to take when tracing and so tracing can't occur.\n",
        "\n",
        "**TLDR**: JAX tracing doesn't work well with conditional statements (\"if ...\"). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLswU8aMEQ9K"
      },
      "source": [
        "To solve this, we have two options:\n",
        "- Use static arguments to make sure JAX traces on a concrete value level - this is not ideal if you need to retrace a lot. Example - bottom of this [section](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-jit).\n",
        "- Use builtin JAX condition flow primitives such as [`lax.cond`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html) or [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8k4R7daBpP"
      },
      "source": [
        "**Exercise 1.3 - Code Task** : Let's convert our ReLU function above to work with jit.\n",
        "\n",
        "**Useful methods:**  [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) (or [`jnp.maximum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.maximum.html), if you prefer.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-4mXLwqaK-b"
      },
      "outputs": [],
      "source": [
        "# Implement a jittable ReLU\n",
        "def relu(x):\n",
        "    # TODO Implement ME!\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5fq_QRoaaG5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run to test your function.\n",
        "check_relu_function(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLtBaplGxlS3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def relu(x):\n",
        "    return jnp.where(x > 0, x, 0)\n",
        "    # Another option - return jnp.maximum(x,0)\n",
        "\n",
        "\n",
        "check_relu_function(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYogDOCLiLXN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Now let's see the performance benefit of using jit! (Run me)\n",
        "\n",
        "# jit our function\n",
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "# generate random input\n",
        "key = jax.random.PRNGKey(42)\n",
        "num_random_numbers = 1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "# time normal jit function\n",
        "jax_time = %timeit -o -n 10 relu(x).block_until_ready()\n",
        "\n",
        "# Warm up/Compile - first run for jitted function\n",
        "relu_jit(x).block_until_ready()\n",
        "\n",
        "# time jitted function\n",
        "jax_jit_time = %timeit -o -n 10 relu_jit(x).block_until_ready()\n",
        "\n",
        "# Let's plot the performance difference\n",
        "jax_avg_time = np.mean(jax_time.all_runs)\n",
        "jax_jit_avg_time = np.mean(jax_jit_time.all_runs)\n",
        "data = {\"JAX (no jit)\": jax_avg_time, \"JAX (with jit)\": jax_jit_avg_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken for ReLU function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxq-z-xzs40s"
      },
      "source": [
        "## Automatic gradient computation\n",
        "\n",
        "The `grad` transformation is used to automatically compute the gradient of a function in JAX. It can be applied to Python and NumPy functions, which means you can differentiate through loops, branches, recursion, and closures.  \n",
        "\n",
        "`grad` takes in a function `f` and returns a function. If `f` is a mathematical function $f(x)$, then `grad(f(x))` corresponds to $f'(x)=\\frac{df}{dx}$. Then `grad(f)(x_0)` yields $f'(x_0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C49R8EOs-GHe"
      },
      "source": [
        "Let's take a simple function $f(x)=6x^4-9x+4$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUMepl6J-dQP"
      },
      "outputs": [],
      "source": [
        "f = lambda x: 6 * x**4 - 9 * x + 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ayvrkpiBiu4"
      },
      "source": [
        "We can compute the gradient of this function - $f'(x)$ and evaluate it at $x=3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNm9hS2S-vJk"
      },
      "outputs": [],
      "source": [
        "dfdx = grad(f)\n",
        "dfdx_3 = dfdx(3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcRUywsnF3LZ"
      },
      "source": [
        "**Exercise 1.4 - Math Task**: Can you calculate $f'(2)$ by hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PybYK6NEFWrD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "answer = 0  # @param {type:\"integer\"}\n",
        "\n",
        "dfdx_2 = dfdx(2.0)\n",
        "\n",
        "assert (\n",
        "    answer == dfdx_2\n",
        "), \"Incorrect answer, hint https://en.wikipedia.org/wiki/Power_rule#Statement_of_the_power_rule\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!') \n",
        "%%latex                                                  \n",
        "\\begin{aligned}\n",
        "f(x) & = 6x^4-9x+4 \\\\\n",
        "f'(x) & = 24x^3 -9  && \\triangleright \\textrm{Power Rule.}  \\\\ \n",
        "f'(2) &  = 24(2)^3 -9 = 183 && \\triangleright \\textrm{Substituting x=2} \\\\\n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "CAwlhxIlRPp9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcB5ZjojH67Q"
      },
      "source": [
        "We can also chain `grad` to calculate higher order deratives. \n",
        "\n",
        "We can calculate $f'''(x)$ as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "013SFq7BE54W"
      },
      "outputs": [],
      "source": [
        "d3dx = grad(grad(grad(f)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_r9VQGoIsa6"
      },
      "source": [
        "**Exercise 1.5 - Math Task**: How about $f'''(2)$ by hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WZUArv4TInPg"
      },
      "outputs": [],
      "source": [
        "answer = 0  # @param {type:\"integer\"}\n",
        "\n",
        "d3dx_2 = d3dx(2.0)\n",
        "\n",
        "assert answer == d3dx_2, \"Incorrect answer, hint ...\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!') \n",
        "%%latex \n",
        "\n",
        "\\begin{aligned}\n",
        "f(x) & = 6x^4-9x+4 \\\\\n",
        "f'(x) & = 24x^3 -9  && \\triangleright \\textrm{Power Rule.}  \\\\\n",
        "f''(x) & = 72x^2  && \\triangleright \\textrm{Power Rule.}  \\\\\n",
        "f'''(x) & = 144x && \\triangleright \\textrm{Power Rule.} \\\\\n",
        "f'''(2) & = 144(2)=288 && \\triangleright \\textrm{Substituting x=2} \\\\ \n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "TCC7SkH8MMVk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3QgJNU9XYyz"
      },
      "source": [
        "Another useful method is `value_and_grad`, where we can get the value ($f(x)$) and gradient ($f'(x)$). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3zeSv6gXuyd"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "f_x, dy_dx = value_and_grad(f)(2.0)\n",
        "print(f\"f(x): {f_x} f‚Ä≤(x): {dy_dx} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> For partial derivatives, you need to use the [`argnums`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) param to specify which variables you want to differentiate with respect to. \n",
        "\n"
      ],
      "metadata": {
        "id": "_vUr-B6gSxnu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MktOLPnwvnH3"
      },
      "source": [
        "**Exercise 1.6 - Group Task:** Chat with neighbour/think about how JAX's automatic differentiation compares to other libraries such as Pytorch or Tensorflow. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful application related to `grad` is when you want your `grad` function to return auxiliary (extra) data, that you don't want differentiated. You can use the `has_aux` parameter to do this (example in \"Auxiliary data\" section in [here](https://github.com/google/jax/blob/main/docs/jax-101/01-jax-basics.ipynb))."
      ],
      "metadata": {
        "id": "rvXlE7z02M2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure Functions\n",
        "\n",
        "JAX transformation and compilation are designed to work reliably on **pure functions** (see [Wikipedia page on pure functions](https://en.wikipedia.org/wiki/Pure_function)). These functions have the following properties:\n",
        "1. All **input** data is passed through the **function's parameters**. \n",
        "2. All **results** are output through the **function's return**. \n",
        "3. The function always returns the same **result** if invoked with the **same inputs**. What if your function involves randomness? Pass in the random seed!\n",
        "4. **No [side-effects](https://en.wikipedia.org/wiki/Side_effect_(computer_science))** - no mutation of non-local variables or input/output streams.  \n",
        " \n",
        "\n",
        "Let's see what could happen if we don't stick to using pure functions."
      ],
      "metadata": {
        "id": "fT56qxXzTVKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Side Effects"
      ],
      "metadata": {
        "id": "Mad7l7s0CtT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call print within a function."
      ],
      "metadata": {
        "id": "xkQWTE2Xe955"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impure_print_side_effect(x):\n",
        "    print(\"Print me!\")  # This is a side-effect\n",
        "    return x\n",
        "\n",
        "\n",
        "# The side-effects appear during the first run\n",
        "print(\"First call: \", jax.jit(impure_print_side_effect)(4.0))"
      ],
      "metadata": {
        "id": "S9aeUdUoBmCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the print statement is called.\n",
        "\n",
        "Let's call this function again. "
      ],
      "metadata": {
        "id": "nu4rnyS7ox_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
        "# This is because JAX now invokes a cached compilation of the function\n",
        "print(\"Second call: \", jax.jit(impure_print_side_effect)(5.0))"
      ],
      "metadata": {
        "id": "-wnkIqAxfDeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ah, no print statement! Since JAX cached the compilation of the function, `print()` calls will only happen during tracing and not every time the function is called. "
      ],
      "metadata": {
        "id": "64rNvVnwo-eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "print(\n",
        "    \"Third call, different type: \", jax.jit(impure_print_side_effect)(jnp.array([5.0]))\n",
        ")"
      ],
      "metadata": {
        "id": "Mp_CkOL-o86t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we called the function with a different shaped object and so it triggered the re-tracing of the function and print was called again. "
      ],
      "metadata": {
        "id": "XFogrIf5fbLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print values in compiled functions, use [host callbacks](https://jax.readthedocs.io/en/latest/jax.experimental.host_callback.html?highlight=print#jax.experimental.host_callback.id_print)([example](https://github.com/google/jax/issues/196#issuecomment-1191155679)) or if your jax version>=0.3.16, you can use [`jax.debug.print`](https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html). \n"
      ],
      "metadata": {
        "id": "pqV6_25GCxHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global variables"
      ],
      "metadata": {
        "id": "EqL1-TGaC8Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using global variables can also lead to some undesired consequences!"
      ],
      "metadata": {
        "id": "t8dzJog8tMe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = 0.0\n",
        "\n",
        "\n",
        "def impure_uses_globals(x):\n",
        "    return x + g\n",
        "\n",
        "\n",
        "# JAX captures the value of the global during the first run\n",
        "print(\"First call: \", jax.jit(impure_uses_globals)(4.0))"
      ],
      "metadata": {
        "id": "vwAkKrDiCXO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints 4, using the original value of `g`.\n",
        "\n",
        "Let's update `g` and call our function again."
      ],
      "metadata": {
        "id": "pWNE8B5btcfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = 10.0  # Update the global\n",
        "\n",
        "# Subsequent runs may silently use the cached value of the globals\n",
        "print(\"Second call: \", jax.jit(impure_uses_globals)(4.0))"
      ],
      "metadata": {
        "id": "mLMpdQZwtUEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we updated our global variable, this still prints 4, using the original value of `g`. This is because the value of `g` was cached."
      ],
      "metadata": {
        "id": "o3-ygEx0tpBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "# This will end up reading the latest value of the global\n",
        "print(\"Third call, different type: \", jax.jit(impure_uses_globals)(jnp.array([4.0])))"
      ],
      "metadata": {
        "id": "LDecWNyktWDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the side-effects example, re-tracing gets triggered when the shape of our input has changed. In this case, our function now uses the updated value of `g`."
      ],
      "metadata": {
        "id": "3mIZaXOqt5ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the global variables are cached, it is still okay to use global **constants** inside jax functions."
      ],
      "metadata": {
        "id": "aLis2BV04BQK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCUB9YkCnCFb"
      },
      "source": [
        "## Function vectorization with vmap\n",
        "\n",
        "vmap (Vectorizing map) automatically vectorizes your python functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e858lqfYKd4d"
      },
      "source": [
        "Let's define a simple function that calculates the min and max of an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6qalyXgDsKB"
      },
      "outputs": [],
      "source": [
        "def min_max(x):\n",
        "    return jnp.array([jnp.min(x), jnp.max(x)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muSIsUkgKlxh"
      },
      "source": [
        "We can apply this function to the vector - `[0, 1, 2, 3, 4]` and get the min and max values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5wIeGieKsWG"
      },
      "outputs": [],
      "source": [
        "x = jnp.arange(5)\n",
        "min_max(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PkC7NnPLNXq"
      },
      "source": [
        "What about if we want to apply this to a batch/list of vectors (i.e. calculate the min and max independently across multiple batches)? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRngFfwCMHLd"
      },
      "source": [
        "Let's create our batch - 3 vectors of size 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKuh459OD6jx"
      },
      "outputs": [],
      "source": [
        "batch_size = 3\n",
        "batched_x = np.arange(15).reshape((batch_size, -1))\n",
        "print(batched_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hApYpVEvNS1y"
      },
      "source": [
        "**Exercise 1.7 - Question**: What do you think would be the result if we passed batch_x into `min_max`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu6C3J0kMrtj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "batch_min_max_output = [[0,4],[5,9],[10,14]]  # @param [\"[[0,4],[5,9],[10,14]]\", \"[[0,10],[1,11],[2,12],[3,13],[4,14]]\", \"[0,14]\"] {type:\"raw\"}\n",
        "\n",
        "assert (batch_min_max_output == np.array(min_max(batched_x))).all(), \"Incorrect answer.\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K0weiHOOb8L"
      },
      "source": [
        "So the above is not what we want. The `min` and `max` is applied across the entire batch, when we want the min and max per vector/mini-batch. \n",
        "\n",
        "We can also manually batch this by `jnp.stack` and a for loop, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8RdAqr8N-Fd"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def manual_batch_min_max_loop(batched_x):\n",
        "    min_max_result_list = []\n",
        "    for x in batched_x:\n",
        "        min_max_result_list.append(min_max(x))\n",
        "    return jnp.stack(min_max_result_list)\n",
        "\n",
        "\n",
        "print(manual_batch_min_max_loop(batched_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmu3VVtMR0GV"
      },
      "source": [
        "Or, just manually updating the `axis` in `jnp.min` and `jnp.max`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzxmORv-RcUg"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def manual_batch_min_max_axis(batched_x):\n",
        "    return jnp.array([jnp.min(batched_x, axis=1), jnp.max(batched_x, axis=1)]).T\n",
        "\n",
        "\n",
        "print(manual_batch_min_max_axis(batched_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CetKYASUSE4Q"
      },
      "source": [
        "These approaches both work, but we need to change our function to work with batches. We can't just run the same code across a batch of data.\n",
        "\n",
        "There is where `vmap` becomes useful! Using `vmap` we can write a function once, as if it is working on a single element, and then use `vmap` to automatically vectorize it! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2F8WUNQROkQ"
      },
      "outputs": [],
      "source": [
        "# define our vmap function using our original single vector function\n",
        "@jit\n",
        "def min_max_vmap(batched_x):\n",
        "    return vmap(min_max)(batched_x)\n",
        "\n",
        "\n",
        "# Run it on a single vecor\n",
        "## We add extra dimention in a single vector, shape changes from (5,) to (1,5), which makes the vmapping possible\n",
        "x_with_leading_dim = jax.numpy.expand_dims(x, axis=0)\n",
        "print(f\"Single vector: {min_max_vmap(x_with_leading_dim)}\")\n",
        "\n",
        "# Run it on batch of vectors\n",
        "print(f\"Batch/list of vector:{min_max_vmap(batched_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bome92VRL6"
      },
      "source": [
        "So this is really convenient, but what about performance? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1Nb4uniUUor"
      },
      "outputs": [],
      "source": [
        "batched_x = np.arange(50000).reshape((500, 100))\n",
        "\n",
        "# Trace the functions with first call\n",
        "manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "min_max_forloop_time = %timeit -o -n 10 manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "min_max_axis_time = %timeit -o -n 10 manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap_time = %timeit -o -n 10 min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "print(\n",
        "    f\"Avg Times (lower is better) - Naive Implementation: {np.round(np.mean(min_max_forloop_time.all_runs),5)} Manually Vectorized: {np.round(np.mean(min_max_axis_time.all_runs),5)} Vmapped Function: {np.round(np.mean(min_max_vmap_time.all_runs),5)} \"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYL758zCYsrR"
      },
      "source": [
        "So `vmap` should be similar in performance to manually vectorized code (if everything is implemented well), and much better than naively vectorized code (i.e. for loops). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAO9dOdrtiqI"
      },
      "source": [
        "## Paralelization with pmap\n",
        "\n",
        "üí°**For this subsection, please ensure that colab is using a `TPU` runtime. If no `TPU` runtimes are available, select `Harware Accelerator` - `None` for a cpu runtime.** \n",
        "\n",
        "With `pmap` we can convert a function written for a single device to a function that can run in parallel across many devices. \n",
        "\n",
        "**Difference between `vmap` and `pmap`**:\n",
        "\n",
        "So both `pmap` and `vmap` transform a function to work over an array, but they differ in implementation. `vmap` adds an extra batch dimension to all the operations in a function, while `pmap` replicates the function and executes each replica on its own XLA device in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gUYA277soR-0"
      },
      "outputs": [],
      "source": [
        "# @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qhlBnLs6AYL"
      },
      "source": [
        "Let's try and `pmap` a batch of dot products.\n",
        "\n",
        "Here is an illustration of how we would typically do this sequentially: \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fz1i2AwA5_7J"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of Sequential Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/not_parallel-2.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTmWNFZ08f8n"
      },
      "source": [
        "Here is the code implementation of this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqTuMldJ9Uv5"
      },
      "outputs": [],
      "source": [
        "# Let's generate a batch of size 8, each with a matrix of size (500, 600)\n",
        "\n",
        "# Let create 8 keys, 1 for each batch\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "\n",
        "# Let create our batches\n",
        "mats = jnp.stack([jax.random.normal(key, (500, 600)) for key in keys])\n",
        "\n",
        "\n",
        "def dot_product_sequential():\n",
        "    @jit\n",
        "    def avg_dot_prod(mats):\n",
        "        result = []\n",
        "        # Loop through batch and compute dp\n",
        "        for mat in mats:\n",
        "            # dot product between the a mat and mat.T (transposed version)\n",
        "            result.append(jnp.dot(mat, mat.T))\n",
        "        return jnp.stack(result)\n",
        "\n",
        "    avg_dot_prod(mats).block_until_ready()\n",
        "\n",
        "\n",
        "run_sequential = %timeit -o -n 5 dot_product_sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBEtecJX-0AW"
      },
      "source": [
        "Here is an illustration of how we would do this in parallel \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uswxurmn-5oC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of Parallel Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/parallelized.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGsq8iTA_N9U"
      },
      "source": [
        "Here is code implementation of batched dot products:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ygFWDfQIoeC"
      },
      "source": [
        "First, we will create `8` random matrices (one for each available tpu devices - colab tpu's have 8 available [devices](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) or the 8 cpu cores as we configured)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZLMx06_K_qR"
      },
      "outputs": [],
      "source": [
        "# Let create 8 keys, 1 for each batch\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "\n",
        "# Each replicated pmapped function get a different key\n",
        "mats = pmap(lambda key: jax.random.normal(key, (500, 600)))(keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BkMsaOtLISj"
      },
      "source": [
        "The leading dimension here needs to equal the dimension of available devices (since we are sending a batch to each device)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWrdv_2wLG4T"
      },
      "outputs": [],
      "source": [
        "print(mats.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnqblcUsLaKZ"
      },
      "source": [
        "Using `pmap` to generate the batches ensures these batches are of type `ShardedDeviceArray`. This is similar to an ndarray, except each batch/shared is stored in the memory of multiple devices, so they can be used in subsequent `pmap` operations without moving data around between devices (GPU/TPU) and hosts (cpu). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAeaBCvcLQWg"
      },
      "outputs": [],
      "source": [
        "print(type(mats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVz0gOWG9pkr"
      },
      "outputs": [],
      "source": [
        "def dot_product_parallel():\n",
        "\n",
        "    # Run a local matmul on each device in parallel (no data transfer)\n",
        "    result = pmap(lambda x: jnp.dot(x, x.T))(\n",
        "        mats\n",
        "    ).block_until_ready()  # result.shape is (8, 5000, 5000)\n",
        "\n",
        "\n",
        "run_parallel = %timeit -o -n  5 dot_product_parallel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64gfyF3ENQzU"
      },
      "source": [
        "It is simple as that. Our dot product now runs in parallel across available devices (cpu, gpus or tpus). As we have more cores/devices, this code will automatically scale! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qcQXSbANP_M",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Let's plot the performance difference (Run Cell)\n",
        "\n",
        "jax_parallel_time = np.mean(run_parallel.all_runs)\n",
        "jax_seq_time = np.mean(run_sequential.all_runs)\n",
        "\n",
        "\n",
        "data = {\"JAX (seq)\": jax_seq_time, \"JAX (parallel - pmap)\": jax_parallel_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken for Seq vs Parallel Dot Product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0j8iJRFUz6v"
      },
      "source": [
        "For some problems, the speed can be directly proportional to the number of devices -- $Nx$ speed up for $N$ devices! \n",
        "\n",
        "We showed an example of using `pmap` for *pure* parallelism, where there is no communication between devices. JAX also has various operations for communication across distributed devices ( more on this [here](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#communication-between-devices).)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "# **Conclusion**\n",
        "**Summary:**\n",
        "- JAX combines Autograd and XLA to perform **accelerated** üöÄ numerical computations. These computations are achieved using transforms such as `jit`,`grad`,`vmap` and `pmap`.\n",
        "- JAX's `grad` function automatically calculates the gradients of your functions for you! \n",
        "- Gradient descent is an effective algorithm to learn linear models, but also more complicated models, where analytical solutions don't exist. \n",
        "- We need to be careful not to over-fit or under-fit on our datasets. \n",
        "- Haiku and Optax make training JAX models more convenient.  \n",
        "\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "* If you are interested in going deeper into optimization, check out [Optimization Practical](https://github.com/khipu-ai/practicals-2023/blob/main/practicals/optimization_and_haiku.ipynb).\n",
        "\n",
        "\n",
        "**References:** \n",
        "\n",
        "Part 1 \n",
        "1. Various JAX [docs](https://jax.readthedocs.io/en/latest/) - specifically [quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html), [common gotchas](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html), [jitting](\n",
        "https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#), [random numbers](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) and [pmap](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html?highlight=pmap#). \n",
        "2. http://matpalm.com/blog/ymxb_pod_slice/\n",
        "3. https://roberttlange.github.io/posts/2020/03/blog-post-10/\n",
        "4. [Machine Learning with JAX - From Zero to Hero | Tutorial #1](https://www.youtube.com/watch?v=SstuvS-tVc0). \n",
        "\n",
        "\n",
        "For other Khipu practicals  visit the [Khipu Practicals GitHub Repository](https://github.com/khipu-ai/practicals-2023)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}